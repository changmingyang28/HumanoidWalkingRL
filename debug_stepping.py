#!/usr/bin/env python3
"""
è°ƒè¯•steppingä»»åŠ¡
"""
import torch
import pickle
import numpy as np
from pathlib import Path
from envs.h1 import H1Env

def debug_stepping():
    # åŠ è½½æ¨¡å‹
    model_dir = "experiments/h1_step_20250821_001704"
    actor_path = Path(model_dir, "actor_19999.pt")
    pkl_path = Path(model_dir, "experiment.pkl")
    
    print("ğŸ“ åŠ è½½é…ç½®...")
    run_args = pickle.load(open(pkl_path, "rb"))
    print(f"ç¯å¢ƒ: {run_args.env}")
    print(f"é…ç½®æ–‡ä»¶: {getattr(run_args, 'yaml', 'None')}")
    
    print("\nğŸ“ åŠ è½½æ¨¡å‹...")
    policy = torch.load(actor_path, weights_only=False, map_location='cpu')
    policy.eval()
    print(f"æ¨¡å‹ç±»å‹: {type(policy)}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in policy.parameters())}")
    
    print("\nğŸŒ åˆ›å»ºç¯å¢ƒ...")
    env = H1Env()
    print(f"ç¯å¢ƒç±»å‹: {type(env)}")
    print(f"åŠ¨ä½œç©ºé—´: {env.action_space.shape if hasattr(env.action_space, 'shape') else len(env.action_space)}")
    print(f"è§‚æµ‹ç©ºé—´: {env.observation_space.shape if hasattr(env.observation_space, 'shape') else len(env.observation_space)}")
    
    print("\nğŸ”„ é‡ç½®ç¯å¢ƒ...")
    obs = env.reset()
    print(f"åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"åˆå§‹è§‚æµ‹èŒƒå›´: [{obs.min():.3f}, {obs.max():.3f}]")
    
    print("\nğŸ¯ æ£€æŸ¥ä»»åŠ¡é…ç½®...")
    if hasattr(env, 'task'):
        task = env.task
        print(f"ä»»åŠ¡ç±»å‹: {type(task)}")
        
        if hasattr(task, 'sequence'):
            print(f"æ­¥è¿›åºåˆ—é•¿åº¦: {len(task.sequence) if task.sequence is not None else 'None'}")
            if task.sequence is not None and len(task.sequence) > 0:
                print(f"ç¬¬ä¸€ä¸ªç›®æ ‡: {task.sequence[0]}")
        
        if hasattr(task, 't1') and hasattr(task, 't2'):
            print(f"å½“å‰ç›®æ ‡ç´¢å¼•: t1={task.t1}, t2={task.t2}")
    
    print("\nğŸ¤– æµ‹è¯•ç­–ç•¥...")
    for step in range(5):
        print(f"\n--- æ­¥éª¤ {step+1} ---")
        
        # è·å–åŠ¨ä½œ
        with torch.no_grad():
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            action = policy.forward(obs_tensor, deterministic=True).detach().numpy()
        
        print(f"åŠ¨ä½œå½¢çŠ¶: {action.shape}")
        print(f"åŠ¨ä½œèŒƒå›´: [{action.min():.3f}, {action.max():.3f}]")
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, done, info = env.step(action)
        
        print(f"å¥–åŠ±: {reward:.4f}")
        print(f"å®Œæˆ: {done}")
        print(f"å¥–åŠ±è¯¦æƒ…: {info if isinstance(info, dict) else 'N/A'}")
        
        # æ£€æŸ¥æœºå™¨äººçŠ¶æ€
        if hasattr(env, 'interface'):
            root_pos = env.interface.get_qpos()[:3]
            print(f"æœºå™¨äººä½ç½®: [{root_pos[0]:.3f}, {root_pos[1]:.3f}, {root_pos[2]:.3f}]")
        
        if done:
            print("ğŸ”„ ç¯å¢ƒé‡ç½®")
            obs = env.reset()
    
    print("\nâœ… è°ƒè¯•å®Œæˆ")

if __name__ == "__main__":
    debug_stepping()