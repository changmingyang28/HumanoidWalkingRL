import sys
sys.path.append('.')
import torch
import numpy as np
from envs.jvrc import JvrcStepEnv

print("ğŸ“Š JVRCæ­¥è¿›æœºå™¨äººæ€§èƒ½æµ‹è¯•...")

# åŠ è½½æ¨¡å‹
actor = torch.load('./experiments/jvrc_step/actor.pt', map_location='cpu')
actor.eval()

env = JvrcStepEnv()
results = []

for i in range(5):  # æµ‹è¯•5ä¸ªepisode
    obs = env.reset()
    total_reward = 0
    steps = 0
    successful_steps = 0
    
    print(f"\nğŸ§ª æµ‹è¯•Episode {i+1}")
    
    for step in range(1000):
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            action = actor(obs_tensor).numpy()[0]
        
        obs, reward, done, info = env.step(action)
        total_reward += reward
        steps += 1
        
        # è®¡ç®—æˆåŠŸçš„æ­¥è¿›
        if reward > 0:
            successful_steps += 1
        
        if step % 200 == 0:
            print(f"  è¿›åº¦: {step}/1000, å½“å‰æ€»å¥–åŠ±: {total_reward:.2f}")
        
        if done:
            break
    
    success_rate = successful_steps / steps if steps > 0 else 0
    results.append((steps, total_reward, success_rate))
    
    print(f"  å®Œæˆ: {steps} æ­¥, æ€»å¥–åŠ±: {total_reward:.2f}, æˆåŠŸç‡: {success_rate:.1%}")

# ç»Ÿè®¡ç»“æœ
print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
avg_steps = np.mean([r[0] for r in results])
avg_reward = np.mean([r[1] for r in results])
avg_success = np.mean([r[2] for r in results])

print(f"å¹³å‡episodeé•¿åº¦: {avg_steps:.1f} æ­¥")
print(f"å¹³å‡æ€»å¥–åŠ±: {avg_reward:.2f}")
print(f"å¹³å‡æˆåŠŸç‡: {avg_success:.1%}")
print(f"æœ€é•¿episode: {max(r[0] for r in results)} æ­¥")
print(f"æœ€é«˜æ€»å¥–åŠ±: {max(r[1] for r in results):.2f}")
